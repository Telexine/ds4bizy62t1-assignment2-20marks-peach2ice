{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraby & Init url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# display progress bar (tqdm>=4.23.4 | pandas==0.24.0)\n",
    "from tqdm import tqdm_notebook as tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init request\n",
    "baseUrl = \"http://www.it.kmitl.ac.th/~teerapong/news_archive\"\n",
    "homeUrl = \"http://www.it.kmitl.ac.th/~teerapong/news_archive/index.html\"\n",
    "response = requests.get(homeUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Month URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse html\n",
    "soup = BeautifulSoup(response.text, \"html.parser\", from_encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create month url\n",
    "li_group = soup.findAll('li')\n",
    "month_url_group = [f\"{baseUrl}/{li.find('a')['href']}\" for li in li_group]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Article Properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: 1408\n",
      "Title: 1408\n",
      "Url: 1408\n"
     ]
    }
   ],
   "source": [
    "# use url each month to fetch article\n",
    "article_category = []\n",
    "article_title = []\n",
    "article_url = []\n",
    "\n",
    "# loop each month\n",
    "for month in month_url_group:\n",
    "    \n",
    "    # init soup\n",
    "    month_resp = requests.get(month);\n",
    "    soup = BeautifulSoup(month_resp.text, \"html.parser\", from_encoding=\"utf-8\")\n",
    "    \n",
    "    # append category\n",
    "    category_group = soup.findAll('td', {'class': 'category'});\n",
    "    for category in category_group:\n",
    "        # article category not available -> skip\n",
    "        if category.getText().strip() == \"N/A\": continue\n",
    "        article_category.append(category.getText().strip())\n",
    "        \n",
    "    # append title & url\n",
    "    title_group = soup.findAll('td', {'class': 'title'});\n",
    "    for title in title_group:\n",
    "        # article title not available -> skip\n",
    "        if title.getText().strip() == \"Article no longer available in archive\": continue\n",
    "        article_title.append(title.getText().strip())\n",
    "        article_url.append(f\"{baseUrl}/{title.find('a')['href']}\")\n",
    "\n",
    "        \n",
    "# display article properties length (check length is match)\n",
    "print(\"Category: \" + str(len(article_category)))\n",
    "print(\"Title: \" + str(len(article_title)))\n",
    "print(\"Url: \" + str(len(article_url)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Article Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e3d8eebf5a4927976e489890dd2999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Content: 100\n"
     ]
    }
   ],
   "source": [
    "# use articule url to fetch article content\n",
    "article_content = []\n",
    "\n",
    "# init progress bar\n",
    "with tqdm(total=len(my_list)) as pbar:\n",
    "    \n",
    "    # loop each article\n",
    "    for article in article_url[:100]:\n",
    "\n",
    "        article_resp = requests.get(article);\n",
    "        soup = BeautifulSoup(article_resp.text, \"html.parser\", from_encoding=\"utf-8\")\n",
    "        \n",
    "        current_content = []\n",
    "        \n",
    "        article_group = soup.findAll('p')\n",
    "        for content in article_group[:-1]:\n",
    "\n",
    "            # check <p> is empty?\n",
    "            if (content.text == \"\"): continue\n",
    "            current_content.append(content.text.rstrip(\"\\n\\r\"))\n",
    "\n",
    "        # join each <p> to raw string and append to article_content\n",
    "        article_content.append(''.join(current_content))\n",
    "        \n",
    "        # update progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "# display article content length\n",
    "print(\"Content: \" + str(len(article_content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write article_title.txt\n",
    "with open(\"./datastore/article_title.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for row in article_title:\n",
    "        file.write(\"%s\\n\" % row)\n",
    "    file.close()\n",
    "    \n",
    "# write article_content.txt\n",
    "with open(\"./datastore/article_content.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for row in article_content:\n",
    "        file.write(\"%s\\n\" % row)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read article_title.txt\n",
    "with open(\"./datastore/article_title.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    title_raw = file.read().splitlines()\n",
    "    file.close()\n",
    "    \n",
    "# read article_title.txt\n",
    "with open(\"./datastore/article_content.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content_raw = file.read().splitlines()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init article DataFrame\n",
    "df_title = pd.DataFrame(title_raw)\n",
    "df_content = pd.DataFrame(content_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      0\n",
      "0     21st-Century Sports: How Digital Technology Is...\n",
      "1                      Asian quake hits European shares\n",
      "2                        BT offers free net phone calls\n",
      "3                     Barclays shares up on merger talk\n",
      "4                      Barkley fit for match in Ireland\n",
      "5                                Bellamy under new fire\n",
      "6                     Benitez 'to launch Morientes bid'\n",
      "7                     Benitez delight after crucial win\n",
      "8                           Big war games battle it out\n",
      "9                     British Library gets wireless net\n",
      "10                    Brizzel to run AAA's in Sheffield\n",
      "11                      Bush budget seeks deep cutbacks\n",
      "12                       Bush to get 'tough' on deficit\n",
      "13                         Cable offers video-on-demand\n",
      "14                     Cabs collect mountain of mobiles\n",
      "15                       Camera phones are 'must-haves'\n",
      "16                      Card fraudsters 'targeting web'\n",
      "17                    Cash gives way to flexible friend\n",
      "18                     Cebit opens to mobile music tune\n",
      "19                      Charvis set to lose fitness bid\n",
      "20                                 Chelsea hold Arsenal\n",
      "21                     Christmas sales worst since 1981\n",
      "22                    Clijsters set for February return\n",
      "23                                     Clyde 0-5 Celtic\n",
      "24                     Coach Ranieri sacked by Valencia\n",
      "25                    Confusion over high-definition TV\n",
      "26                       Cuba winds back economic clock\n",
      "27                              DS aims to touch gamers\n",
      "28                    Dawson set for new Wasps contract\n",
      "29                           Diageo to buy US wine firm\n",
      "...                                                 ...\n",
      "1378                   Sony PSP tipped as a 'must-have'\n",
      "1379                   Stevens named in England line-up\n",
      "1380                   T-Mobile bets on 'pocket office'\n",
      "1381                   Takeover offer for Sunderland FC\n",
      "1382                          The future in your pocket\n",
      "1383                                 The gloves are off\n",
      "1384                    Tigers wary of Farrell 'gamble'\n",
      "1385                  Troubled Marsh under SEC scrutiny\n",
      "1386                  UK gets official virus alert site\n",
      "1387                    US blogger fired by her airline\n",
      "1388                    US company admits Benin bribery\n",
      "1389                  US seeks new $280bn smoker ruling\n",
      "1390                      US woman sues over cartridges\n",
      "1391                           Uefa approves fake grass\n",
      "1392                   Unclear future for striker Baros\n",
      "1393                    Venezuela reviews foreign deals\n",
      "1394                         Vickery out of Six Nations\n",
      "1395                  Wales make two changes for France\n",
      "1396                   Warning over tsunami aid website\n",
      "1397                       Weak dollar hits Online News\n",
      "1398                   Wenger dejected as Arsenal slump\n",
      "1399               What high-definition will do to DVDs\n",
      "1400                 What's next for next-gen consoles?\n",
      "1401               Why few targets are better than many\n",
      "1402                  Wi-fi web reaches farmers in Peru\n",
      "1403                    Woodward eyes Brennan for Lions\n",
      "1404                  WorldCom trial starts in New York\n",
      "1405                    Yukos accused of lying to court\n",
      "1406                   Yukos drops banks from court bid\n",
      "1407                      Zambia confident and cautious\n",
      "\n",
      "[1408 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
